# leapai/model/node_graph.py è¯¦ç»†åˆ†æ

## ğŸ“‹ æ–‡ä»¶æ¦‚è§ˆ

`leapai/model/node_graph.py` æ˜¯LeapAIæ¡†æ¶çš„æ ¸å¿ƒæ¨¡å‹ç»„ä»¶ï¼Œå®ç°äº†åŸºäºèŠ‚ç‚¹å›¾çš„å¤šä»»åŠ¡è®­ç»ƒç³»ç»Ÿã€‚è¯¥æ–‡ä»¶æä¾›äº†å®Œæ•´çš„èŠ‚ç‚¹ç®¡ç†ã€æ‹“æ‰‘æ‰§è¡Œã€ä¼˜åŒ–å™¨é…ç½®å’Œå¤šä»»åŠ¡åè°ƒåŠŸèƒ½ã€‚

**æ–‡ä»¶è·¯å¾„**: [`leapai/model/node_graph.py`](../leapai/model/node_graph.py)  
**æ–‡ä»¶å¤§å°**: 225è¡Œ  
**æ ¸å¿ƒåŠŸèƒ½**: å¤šä»»åŠ¡èŠ‚ç‚¹å›¾æ¨¡å‹ã€æ‹“æ‰‘æ‰§è¡Œã€ä¼˜åŒ–å™¨ç®¡ç†  

## ğŸ¯ è®¾è®¡ç›®æ ‡

### ä¸»è¦åŠŸèƒ½
1. **èŠ‚ç‚¹å›¾ç®¡ç†**: ç»Ÿä¸€ç®¡ç†å¤šä¸ªä»»åŠ¡èŠ‚ç‚¹
2. **æ‹“æ‰‘æ‰§è¡Œ**: æ”¯æŒä¸åŒé˜¶æ®µçš„æ‹“æ‰‘å‡½æ•°æ‰§è¡Œ
3. **å¤šä»»åŠ¡åè°ƒ**: åè°ƒå¤šä¸ªä»»åŠ¡çš„è®­ç»ƒå’Œæ¨ç†
4. **ä¼˜åŒ–å™¨é…ç½®**: æ”¯æŒåˆ†ç»„å­¦ä¹ ç‡å’Œå¤šä¼˜åŒ–å™¨
5. **æ¢¯åº¦åŒæ­¥**: å®ç°çµæ´»çš„æ¢¯åº¦ç´¯ç§¯å’ŒåŒæ­¥ç­–ç•¥

## ğŸ”§ æ ¸å¿ƒç»„ä»¶åˆ†æ

### 1. NodeGraphç±» (ç¬¬28-74è¡Œ)
```python
@LEAP_OBJECTS.register_module()
class NodeGraph(L.LightningModule):
    def __init__(
        self,
        graph_nodes: Dict[str, nn.Module],
        task_topologies: Dict[str, Callable],
        optimizer_cfg: Dict,
        lr_scheduler_cfg: Dict = None,
        task_loss_weights: Dict[str, float] = None,
        accumulate_grad_batches: int = 1,
        transfer_on_cuda: Dict[str, Dict[str, Callable]] = None,
        warmup_steps: int = 0,
    ) -> None:
```

#### å…³é”®å‚æ•°
- **graph_nodes**: å›¾èŠ‚ç‚¹å­—å…¸ï¼Œé”®ä¸ºèŠ‚ç‚¹åç§°ï¼Œå€¼ä¸ºnn.Module
- **task_topologies**: ä»»åŠ¡æ‹“æ‰‘å­—å…¸ï¼Œé”®ä¸ºä»»åŠ¡åç§°ï¼Œå€¼ä¸ºæ‹“æ‰‘å‡½æ•°
- **optimizer_cfg**: ä¼˜åŒ–å™¨é…ç½®å­—å…¸
- **task_loss_weights**: ä»»åŠ¡æŸå¤±æƒé‡å­—å…¸
- **accumulate_grad_batches**: æ¢¯åº¦ç´¯ç§¯æ‰¹æ¬¡æ•°
- **transfer_on_cuda**: CUDAä¼ è¾“å‡½æ•°é…ç½®
- **warmup_steps**: é¢„çƒ­æ­¥æ•°

### 2. ä¼˜åŒ–å™¨é…ç½® (ç¬¬76-111è¡Œ)
```python
def configure_optimizers(self):
    res = {}
    group_lr_scale = self.optimizer_cfg.pop("group_lr_scale", None)
    
    if group_lr_scale is not None:
        # åˆ†ç»„å­¦ä¹ ç‡é…ç½®
        params = list(self.parameters())
        base_lr = self.optimizer_cfg["lr"]
        optimizer_param_groups = []
        
        for key, lr_scale in group_lr_scale.items():
            optimizer_param_group = deepcopy(self.optimizer_cfg)
            optimizer_param_group["lr"] = base_lr * lr_scale
            optimizer_param_group["params"] = []
            
            for name, param in self.named_parameters():
                if name.startswith(key):
                    optimizer_param_group["params"].append(param)
                    params.remove(param)
                    rank_zero_info(f"submodule: {name},\tlr_mult {lr_scale}")
            
            optimizer_param_groups.append(optimizer_param_group)
        
        default_optimizer_param_group = deepcopy(self.optimizer_cfg)
        default_optimizer_param_group["params"] = params
        optimizer_param_groups.append(default_optimizer_param_group)
        self.optimizer_cfg["params"] = optimizer_param_groups
    else:
        self.optimizer_cfg["params"] = self.parameters()
    
    # æ„å»ºä¼˜åŒ–å™¨
    opt = build_from_cfg(self.optimizer_cfg, LEAP_OBJECTS)
    res["optimizer"] = opt
    
    # æ„å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    if self.lr_scheduler_cfg:
        self.lr_scheduler_cfg["optimizer"] = opt
        lr_scheduler = build_from_cfg(self.lr_scheduler_cfg, LEAP_OBJECTS)
        res["lr_scheduler"] = lr_scheduler
    
    return res
```

### 3. è®­ç»ƒæ­¥éª¤ (ç¬¬127-162è¡Œ)
```python
def training_step(self, batches: dict, batch_idx: int):
    if not isinstance(batches, dict):
        raise TypeError(f"Batches must by `dict` but got {type(batches)}")
    
    opt = self.optimizers()
    need_sync = self._is_grad_sync_step()
    
    log_losses = {}
    start = time.monotonic()
    
    for task_id, (task_name, batch) in enumerate(batches.items()):
        final_task = self._is_final_task(task_id)
        sync_context = self.trainer.model.no_sync
        if need_sync and final_task:
            sync_context = nullcontext
        
        topo_fn = self.task_topologies[task_name]
        with sync_context():
            losses = topo_fn("train", self, batch, batch_idx)
            assert isinstance(losses, (dict, tuple, list, torch.Tensor))
            flat_losses = flat_to_dict(losses, prefix=task_name)
            total_loss = sum(flat_losses.values())
            total_loss = total_loss * self.task_loss_weights[task_name]
            self.manual_backward(total_loss)
            flat_losses = detach_losses(flat_losses)
            log_losses[task_name] = flat_losses
    
    if need_sync:
        opt.step()
        opt.zero_grad(set_to_none=True)
    
    if self.lr_scheduler_cfg and self.global_step > self.warmup_steps:
        self.lr_scheduler_step(self.lr_schedulers(), None)
    
    end = time.monotonic()
    log_losses["modeltime"] = end - start
    log_losses["datatime"] = batches.pop("_data_time_cost", None)
    return log_losses
```

### 4. éªŒè¯æ­¥éª¤ (ç¬¬164-178è¡Œ)
```python
def validation_step(
    self, batches: dict, batch_idx: int, dataloader_idx: int = 0
):
    task_out = {}
    start = time.monotonic()
    
    for task_name, batch in batches.items():
        if batch is None:
            model_outs = None
        else:
            topo_fn = self.task_topologies[task_name]
            model_outs = topo_fn("val", self, batch, batch_idx)
        task_out[task_name] = model_outs
    
    end = time.monotonic()
    task_out["modeltime"] = end - start
    return task_out
```

### 5. é¢„æµ‹æ­¥éª¤ (ç¬¬180-194è¡Œ)
```python
def predict_step(
    self, batch: dict, batch_idx: int, dataloader_idx: int = 0
) -> dict:
    task_out = {}
    if "task_name" in batch.keys():
        task_name = batch["task_name"][0]
    else:
        task_name = self.task_names[0]
    
    if batch is None:
        model_outs = None
    else:
        topo_fn = self.task_topologies[task_name]
        model_outs = topo_fn("predict", self, batch, batch_idx)
    
    task_out[task_name] = model_outs
    return task_out
```

### 6. CUDAä¼ è¾“å¤„ç† (ç¬¬196-214è¡Œ)
```python
def on_after_batch_transfer(self, batch: Any, dataloader_idx: int):
    if self.transfer_on_cuda:
        stage = self.get_stage()
        task_transfer_dict = self.transfer_on_cuda.get(stage, None)
        if task_transfer_dict is None:
            return batch
        
        if stage == "predict":
            task_name = batch["task_name"][0]
            transfer = task_transfer_dict.get(task_name, None)
            if transfer and batch:
                batch = transfer(batch)
        else:
            for task_name, transfer in task_transfer_dict.items():
                if transfer:
                    data = batch[task_name]
                    if transfer and data:
                        data = transfer(data)
                        batch[task_name] = data
    return batch
```

### 7. é˜¶æ®µè¯†åˆ« (ç¬¬216-225è¡Œ)
```python
def get_stage(self):
    trainer = self.trainer
    if trainer.training:
        return "train"
    elif trainer.validating or trainer.sanity_checking:
        return "val"
    elif trainer.predicting:
        return "predict"
    else:
        return "test"
```

## ğŸ¯ å…³é”®è®¾è®¡æ¨¡å¼

### 1. èŠ‚ç‚¹å›¾æ¨¡å¼
- èŠ‚ç‚¹ç®¡ç†: ç»Ÿä¸€ç®¡ç†å¤šä¸ªç¥ç»ç½‘ç»œæ¨¡å—
- æ‹“æ‰‘æ‰§è¡Œ: é€šè¿‡æ‹“æ‰‘å‡½æ•°æ§åˆ¶èŠ‚ç‚¹æ‰§è¡Œé¡ºåº
- æ¨¡å—åŒ–è®¾è®¡: æ¯ä¸ªèŠ‚ç‚¹ç‹¬ç«‹å¯å¤ç”¨

### 2. å¤šä»»åŠ¡åè°ƒæ¨¡å¼
- ä»»åŠ¡å¹¶è¡Œ: å¤šä¸ªä»»åŠ¡åŒæ—¶è®­ç»ƒ
- æŸå¤±åŠ æƒ: ä¸åŒä»»åŠ¡ä½¿ç”¨ä¸åŒæƒé‡
- æ¢¯åº¦åŒæ­¥: æ§åˆ¶å¤šä»»åŠ¡æ¢¯åº¦æ›´æ–°æ—¶æœº

### 3. åˆ†ç»„ä¼˜åŒ–æ¨¡å¼
- å‚æ•°åˆ†ç»„: æ ¹æ®åç§°å‰ç¼€åˆ†ç»„å‚æ•°
- å·®å¼‚åŒ–å­¦ä¹ ç‡: ä¸åŒç»„ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
- åŠ¨æ€æ„å»º: ä½¿ç”¨æ³¨å†Œè¡¨åŠ¨æ€æ„å»ºä¼˜åŒ–å™¨

## ğŸ“Š æ ¸å¿ƒåŠŸèƒ½ç‰¹æ€§

### 1. å¤šä»»åŠ¡åè°ƒ
- **å¹¶è¡Œå¤„ç†**: å¤šä¸ªä»»åŠ¡åŒæ—¶è®­ç»ƒ
- **æŸå¤±åŠ æƒ**: ä¸åŒä»»åŠ¡ä½¿ç”¨ä¸åŒæƒé‡
- **æ¢¯åº¦åŒæ­¥**: æ§åˆ¶å¤šä»»åŠ¡æ¢¯åº¦æ›´æ–°æ—¶æœº
- **æ€§èƒ½ç›‘æ§**: è®°å½•æ¯ä¸ªä»»åŠ¡çš„æ‰§è¡Œæ—¶é—´

### 2. èŠ‚ç‚¹ç®¡ç†
- **æ¨¡å—æ³¨å†Œ**: ç»Ÿä¸€æ³¨å†Œå’Œç®¡ç†èŠ‚ç‚¹æ¨¡å—
- **åç§°éªŒè¯**: é˜²æ­¢èŠ‚ç‚¹åç§°å†²çª
- **ç±»å‹æ£€æŸ¥**: ç¡®ä¿èŠ‚ç‚¹æ˜¯nn.Moduleç±»å‹
- **åŠ¨æ€è®¿é—®**: æ”¯æŒé€šè¿‡åç§°è®¿é—®èŠ‚ç‚¹

### 3. æ‹“æ‰‘æ‰§è¡Œ
- **é˜¶æ®µæ„ŸçŸ¥**: æ ¹æ®è®­ç»ƒ/éªŒè¯/é¢„æµ‹é˜¶æ®µæ‰§è¡Œä¸åŒé€»è¾‘
- **å‡½æ•°è°ƒç”¨**: é€šè¿‡æ‹“æ‰‘å‡½æ•°æ§åˆ¶èŠ‚ç‚¹æ‰§è¡Œé¡ºåº
- **æ•°æ®æµ**: ç®¡ç†èŠ‚ç‚¹é—´çš„æ•°æ®æµåŠ¨
- **é”™è¯¯å¤„ç†**: å¯¹æ‹“æ‰‘æ‰§è¡Œç»“æœè¿›è¡Œç±»å‹æ£€æŸ¥

### 4. ä¼˜åŒ–å™¨ç®¡ç†
- **åˆ†ç»„å­¦ä¹ ç‡**: æ”¯æŒä¸åŒæ¨¡å—ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡
- **åŠ¨æ€æ„å»º**: ä½¿ç”¨æ³¨å†Œè¡¨åŠ¨æ€æ„å»ºä¼˜åŒ–å™¨
- **è°ƒåº¦å™¨æ”¯æŒ**: é›†æˆå­¦ä¹ ç‡è°ƒåº¦å™¨
- **å‚æ•°åˆ†ç»„**: æ ¹æ®æ¨¡å—åç§°å‰ç¼€åˆ†ç»„å‚æ•°

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### 1. åŸºæœ¬å¤šä»»åŠ¡é…ç½®
```python
# å®šä¹‰å›¾èŠ‚ç‚¹
graph_nodes = {
    "backbone": ResNetBackbone(),
    "neck": FPNNeck(),
    "head": DetectionHead(),
}

# å®šä¹‰ä»»åŠ¡æ‹“æ‰‘
def dynamic_topology(state, model, batch, batch_idx):
    features = model.backbone(batch["images"])
    neck_features = model.neck(features)
    outputs = model.head(neck_features)
    return {"loss": outputs["loss"]}

task_topologies = {
    "dynamic": dynamic_topology,
    "static": static_topology,
}

# åˆ›å»ºNodeGraph
model = NodeGraph(
    graph_nodes=graph_nodes,
    task_topologies=task_topologies,
    optimizer_cfg=optimizer_config,
    task_loss_weights={"dynamic": 2.0, "static": 1.0}
)
```

### 2. åˆ†ç»„å­¦ä¹ ç‡é…ç½®
```python
optimizer_cfg = {
    "type": "AdamW",
    "lr": 1e-3,
    "group_lr_scale": {
        "backbone": 0.1,  # Backboneä½¿ç”¨10%å­¦ä¹ ç‡
        "neck": 0.5,      # Neckä½¿ç”¨50%å­¦ä¹ ç‡
        "head": 1.0,      # Headä½¿ç”¨100%å­¦ä¹ ç‡
    }
}
```

## ğŸ¯ æ ¸å¿ƒä¼˜åŠ¿

### 1. æ¨¡å—åŒ–è®¾è®¡
- **èŠ‚ç‚¹ç‹¬ç«‹**: æ¯ä¸ªèŠ‚ç‚¹å¯ä»¥ç‹¬ç«‹å¼€å‘å’Œæµ‹è¯•
- **æ‹“æ‰‘çµæ´»**: å¯ä»¥çµæ´»å®šä¹‰èŠ‚ç‚¹æ‰§è¡Œé¡ºåº
- **æ˜“äºæ‰©å±•**: æ–°å¢èŠ‚ç‚¹å’Œä»»åŠ¡éƒ½å¾ˆç®€å•
- **ä»£ç å¤ç”¨**: èŠ‚ç‚¹å¯ä»¥åœ¨ä¸åŒä»»åŠ¡é—´å¤ç”¨

### 2. å¤šä»»åŠ¡æ”¯æŒ
- **åŸç”Ÿæ”¯æŒ**: å†…ç½®å¤šä»»åŠ¡è®­ç»ƒæœºåˆ¶
- **æŸå¤±åŠ æƒ**: çµæ´»æ§åˆ¶ä¸åŒä»»åŠ¡çš„é‡è¦æ€§
- **æ¢¯åº¦åè°ƒ**: æ™ºèƒ½çš„æ¢¯åº¦åŒæ­¥ç­–ç•¥
- **æ€§èƒ½ä¼˜åŒ–**: é’ˆå¯¹å¤šä»»åŠ¡çš„æ€§èƒ½ä¼˜åŒ–

### 3. é…ç½®é©±åŠ¨
- **å‚æ•°åŒ–**: æ‰€æœ‰å…³é”®å‚æ•°éƒ½å¯é…ç½®
- **åŠ¨æ€æ„å»º**: ä½¿ç”¨æ³¨å†Œè¡¨åŠ¨æ€æ„å»ºç»„ä»¶
- **çµæ´»è°ƒæ•´**: è¿è¡Œæ—¶å¯ä»¥è°ƒæ•´å‚æ•°
- **å®éªŒå‹å¥½**: ä¾¿äºè¿›è¡Œè¶…å‚æ•°å®éªŒ

## ğŸ“ æœ€ä½³å®è·µ

### 1. èŠ‚ç‚¹è®¾è®¡
```python
class MyNode(nn.Module):
    def __init__(self, config):
        super().__init__()
        # èŠ‚ç‚¹åˆå§‹åŒ–
    
    def forward(self, x):
        # èŠ‚ç‚¹å‰å‘ä¼ æ’­
        return processed_x
```

### 2. æ‹“æ‰‘å‡½æ•°è®¾è®¡
```python
def task_topology(state, model, batch, batch_idx):
    # state: "train", "val", "predict"
    # model: NodeGraphå®ä¾‹
    # batch: å½“å‰æ‰¹æ¬¡æ•°æ®
    # batch_idx: æ‰¹æ¬¡ç´¢å¼•
    
    if state == "train":
        # è®­ç»ƒé€»è¾‘
        return losses
    elif state == "val":
        # éªŒè¯é€»è¾‘
        return outputs
    else:
        # é¢„æµ‹é€»è¾‘
        return predictions
```

## ğŸ‰ æ€»ç»“

`leapai/model/node_graph.py` æ˜¯LeapAIæ¡†æ¶çš„æ ¸å¿ƒæ¨¡å‹ç»„ä»¶ï¼Œæä¾›äº†å®Œæ•´çš„å¤šä»»åŠ¡èŠ‚ç‚¹å›¾è®­ç»ƒç³»ç»Ÿã€‚å®ƒå…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š

### âœ… æ ¸å¿ƒåŠŸèƒ½
1. **èŠ‚ç‚¹å›¾ç®¡ç†**: ç»Ÿä¸€ç®¡ç†å¤šä¸ªç¥ç»ç½‘ç»œèŠ‚ç‚¹
2. **æ‹“æ‰‘æ‰§è¡Œ**: æ”¯æŒä¸åŒé˜¶æ®µçš„æ‹“æ‰‘å‡½æ•°æ‰§è¡Œ
3. **å¤šä»»åŠ¡åè°ƒ**: åè°ƒå¤šä¸ªä»»åŠ¡çš„è®­ç»ƒå’Œæ¨ç†
4. **ä¼˜åŒ–å™¨é…ç½®**: æ”¯æŒåˆ†ç»„å­¦ä¹ ç‡å’Œå¤šä¼˜åŒ–å™¨
5. **æ¢¯åº¦åŒæ­¥**: å®ç°çµæ´»çš„æ¢¯åº¦ç´¯ç§¯å’ŒåŒæ­¥ç­–ç•¥

### ğŸ”§ è®¾è®¡ä¼˜åŠ¿
1. **é«˜åº¦æ¨¡å—åŒ–**: èŠ‚ç‚¹ç‹¬ç«‹ï¼Œæ‹“æ‰‘çµæ´»
2. **å¤šä»»åŠ¡åŸç”Ÿ**: å†…ç½®å¤šä»»åŠ¡è®­ç»ƒæ”¯æŒ
3. **é…ç½®é©±åŠ¨**: å®Œå…¨å‚æ•°åŒ–çš„è®¾è®¡
4. **æ€§èƒ½ä¼˜åŒ–**: é’ˆå¯¹å¤šä»»åŠ¡çš„æ€§èƒ½ä¼˜åŒ–
5. **æ˜“äºæ‰©å±•**: æ–°å¢èŠ‚ç‚¹å’Œä»»åŠ¡ç®€å•

### ğŸ“š å­¦ä¹ ä»·å€¼
é€šè¿‡æ·±å…¥ç†è§£node_graph.pyï¼Œå¯ä»¥æŒæ¡ï¼š
- å¤šä»»åŠ¡ç¥ç»ç½‘ç»œçš„è®¾è®¡æ¨¡å¼
- èŠ‚ç‚¹å›¾æ¶æ„çš„å®ç°æ–¹æ³•
- æ¢¯åº¦ç´¯ç§¯å’ŒåŒæ­¥çš„æœºåˆ¶
- åˆ†ç»„å­¦ä¹ ç‡çš„é…ç½®æ–¹æ³•
- Lightningæ¡†æ¶çš„æ·±åº¦ä½¿ç”¨æŠ€å·§

è¿™ä¸ªç»„ä»¶ä¸ºLeapAIæ¡†æ¶çš„å¤šä»»åŠ¡æ„ŸçŸ¥ç³»ç»Ÿæä¾›äº†å¼ºå¤§çš„åŸºç¡€ï¼Œæ˜¯ç†è§£æ¡†æ¶æ¨¡å‹æ¶æ„çš„é‡è¦å…¥å£ã€‚

## ğŸ“š ç›¸å…³èµ„æº

- **[`leapai/model/node_graph.py`](../leapai/model/node_graph.py)** - æºæ–‡ä»¶ï¼ˆ225è¡Œï¼‰
- **[`leapai/registry.py`](../leapai/registry.py)** - æ³¨å†Œæœºåˆ¶
- **[`projects/perception/entry.py`](../projects/perception/entry.py)** - NodeGraphä½¿ç”¨ç¤ºä¾‹
- **[`projects/perception/configs/lpperception_current_hpa_step1.py`](../projects/perception/configs/lpperception_current_hpa_step1.py)** - é…ç½®ç¤ºä¾‹

é€šè¿‡è¿™äº›è¯¦ç»†çš„å­¦ä¹ èµ„æºï¼Œæ‚¨å¯ä»¥å…¨é¢æŒæ¡LeapAIæ¡†æ¶çš„èŠ‚ç‚¹å›¾æ¨¡å‹æœºåˆ¶ï¼Œä¸ºæ·±å…¥ä½¿ç”¨å’Œæ‰©å±•æ¡†æ¶å¥ å®šåšå®åŸºç¡€ã€‚
