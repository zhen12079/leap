# LeapAIæ¡†æ¶è¯¦ç»†ç½‘ç»œæ¶æ„è¾“å‡º

## ğŸ¯ é…ç½®ç¯å¢ƒ
- **é…ç½®æ–‡ä»¶**: `projects/perception/configs/lpperception_current_hpa_step1.py`
- **å…¥å£æ–‡ä»¶**: `projects/perception/entry.py`
- **è¿è¡Œæ¨¡å¼**: `train`
- **æ‰¹æ¬¡å¤§å°**: 16
- **GPUæ•°é‡**: æ ¹æ®ç¯å¢ƒå˜é‡ç¡®å®š

## ğŸ—ï¸ å®Œæ•´ç½‘ç»œå±‚æ¬¡ç»“æ„

### ğŸ“¸ è¾“å…¥å±‚ (Input Layer)

#### ç›¸æœºè¾“å…¥
```
Camera Input Group 1 (front_narrow):
â”œâ”€â”€ Tensor Shape: [16, 1, 3, 512, 960]
â”œâ”€â”€ Data Type: torch.float32
â”œâ”€â”€ Memory: ~18.8MB per batch
â””â”€â”€ Description: å‰è§†çª„è§’ç›¸æœº

Camera Input Group 2 (front_wide):
â”œâ”€â”€ Tensor Shape: [16, 1, 3, 1024, 1920] â†’ [16, 1, 3, 512, 960]
â”œâ”€â”€ Data Type: torch.float32
â”œâ”€â”€ Memory: ~37.5MB per batch
â””â”€â”€ Description: å‰è§†å¹¿è§’ç›¸æœº

Camera Input Group 3 (back):
â”œâ”€â”€ Tensor Shape: [16, 1, 3, 512, 960]
â”œâ”€â”€ Data Type: torch.float32
â”œâ”€â”€ Memory: ~18.8MB per batch
â””â”€â”€ Description: åè§†ç›¸æœº

Camera Input Group 4 (4 side cameras):
â”œâ”€â”€ front_left: [16, 1, 3, 512, 960]
â”œâ”€â”€ back_left: [16, 1, 3, 512, 960]
â”œâ”€â”€ front_right: [16, 1, 3, 512, 960]
â”œâ”€â”€ back_right: [16, 1, 3, 512, 960]
â”œâ”€â”€ Total Memory: ~75.2MB per batch
â””â”€â”€ Description: å››ä¸ªä¾§è§†ç›¸æœº

Total Camera Input Memory: ~150MB per batch
```

#### LiDARè¾“å…¥
```
LiDAR Input:
â”œâ”€â”€ Raw Points: Variable length (avg ~50,000 points/frame)
â”œâ”€â”€ Point Format: [x, y, z, intensity]
â”œâ”€â”€ Data Type: torch.float32
â”œâ”€â”€ Memory: ~8MB per frame (avg)
â””â”€â”€ Description: æ¿€å…‰é›·è¾¾ç‚¹äº‘æ•°æ®

Voxelized LiDAR:
â”œâ”€â”€ Voxels: [16, 20000, 48, 4]
â”œâ”€â”€ Voxel Coords: [16, 20000, 4]
â”œâ”€â”€ Voxel Num Points: [16, 20000]
â”œâ”€â”€ Memory: ~61MB per batch
â””â”€â”€ Voxel Size: [0.2, 0.2, 8.0]
```

### ğŸ§  ç‰¹å¾æå–éª¨å¹²ç½‘ç»œ (Feature Extraction Backbone)

#### ResNet34V2 - Group1 (front_narrow)
```
stage1_backbone0 (ResNet34V2):
â”œâ”€â”€ Input: [16, 1, 3, 512, 960]
â”œâ”€â”€ Conv1: [16, 1, 64, 256, 480]
â”‚   â”œâ”€â”€ Kernel: 7x7, Stride: 2, Padding: 3
â”‚   â”œâ”€â”€ Parameters: 9,408
â”‚   â””â”€â”€ Output: [16, 1, 64, 256, 480]
â”œâ”€â”€ Layer1 (Residual Blocks):
â”‚   â”œâ”€â”€ Input: [16, 1, 64, 256, 480]
â”‚   â”œâ”€â”€ Output: [16, 1, 64, 256, 480]
â”‚   â”œâ”€â”€ Blocks: 3
â”‚   â””â”€â”€ Parameters: ~70,000
â”œâ”€â”€ Layer2 (Residual Blocks):
â”‚   â”œâ”€â”€ Input: [16, 1, 64, 256, 480]
â”‚   â”œâ”€â”€ Output: [16, 1, 128, 128, 240]
â”‚   â”œâ”€â”€ Blocks: 4
â”‚   â””â”€â”€ Parameters: ~220,000
â”œâ”€â”€ Layer3 (Residual Blocks):
â”‚   â”œâ”€â”€ Input: [16, 1, 128, 128, 240]
â”‚   â”œâ”€â”€ Output: [16, 1, 256, 64, 120]
â”‚   â”œâ”€â”€ Blocks: 6
â”‚   â””â”€â”€ Parameters: ~700,000
â”œâ”€â”€ Layer4 (Residual Blocks):
â”‚   â”œâ”€â”€ Input: [16, 1, 256, 64, 120]
â”‚   â”œâ”€â”€ Output: [16, 1, 512, 32, 60]
â”‚   â”œâ”€â”€ Blocks: 3
â”‚   â””â”€â”€ Parameters: ~500,000
â””â”€â”€ Total Parameters: ~1.5M
```

#### ResNet34V2 - Group2 (front_wide)
```
stage1_backbone1 (ResNet34V2):
â”œâ”€â”€ Input: [16, 1, 3, 512, 960]
â”œâ”€â”€ Output Features: [256, 512] (indices [3,4])   #  è¿™æ˜¯ä»€ä¹ˆå«ä¹‰ï¼Ÿï¼Ÿ
â”œâ”€â”€ Feature Shapes:
â”‚   â”œâ”€â”€ Layer3: [16, 1, 256, 64, 120]
â”‚   â””â”€â”€ Layer4: [16, 1, 512, 32, 60]
â””â”€â”€ Total Parameters: ~21.3M
```

#### ResNet34V2 - Group3 (back)
```
stage1_backbone2 (ResNet34V2):
â”œâ”€â”€ Input: [16, 1, 3, 512, 960]
â”œâ”€â”€ Output Features: [128, 256, 512] (indices [2,3,4])  #  è¿™æ˜¯ä»€ä¹ˆå«ä¹‰ï¼Ÿï¼Ÿ
â”œâ”€â”€ Feature Shapes:
â”‚   â”œâ”€â”€ Layer2: [16, 1, 128, 128, 240]
â”‚   â”œâ”€â”€ Layer3: [16, 1, 256, 64, 120]
â”‚   â””â”€â”€ Layer4: [16, 1, 512, 32, 60]
â””â”€â”€ Total Parameters: ~21.3M
```

#### ResNet34V2 - Group4 (4 side cameras)
```
stage1_backbone3 (ResNet34V2):
â”œâ”€â”€ Input: [16, 4, 3, 512, 960] â†’ [64, 3, 512, 960]  # è¿™é‡Œå¦‚ä½•æ“ä½œ
â”œâ”€â”€ Output Features: [256, 512] (indices [3,4])
â”œâ”€â”€ Feature Shapes:
â”‚   â”œâ”€â”€ Layer3: [64, 1, 256, 64, 120] â†’ [16, 4, 256, 64, 120]  # è¿™é‡Œå¦‚ä½•æ“ä½œ
â”‚   â””â”€â”€ Layer4: [64, 1, 512, 32, 60] â†’ [16, 4, 512, 32, 60]
â””â”€â”€ Total Parameters: ~21.3M
```

### ğŸŒ‰ FPNé¢ˆéƒ¨ç½‘ç»œ (Feature Pyramid Network)

#### FPN Neck - Group1 & Group3  
```
stage1_neck0 & stage1_neck2 (FpnNeck):
â”œâ”€â”€ Input Channels: [128, 256, 512]
â”œâ”€â”€ Output Channels: 256
â”œâ”€â”€ Input Features:
â”‚   â”œâ”€â”€ P2: [16, N, 128, 128, 240]
â”‚   â”œâ”€â”€ P3: [16, N, 256, 64, 120]
â”‚   â””â”€â”€ P4: [16, N, 512, 32, 60]
â”œâ”€â”€ Lateral Convs:
â”‚   â”œâ”€â”€ Conv2d(128â†’256): 3x3, stride=1, padding=1
â”‚   â”œâ”€â”€ Conv2d(256â†’256): 3x3, stride=1, padding=1
â”‚   â””â”€â”€ Conv2d(512â†’256): 3x3, stride=1, padding=1
â”œâ”€â”€ Output Convs:
â”‚   â”œâ”€â”€ Conv2d(256â†’256): 3x3, stride=1, padding=1
â”‚   â””â”€â”€ Conv2d(256â†’256): 3x3, stride=1, padding=1
â”œâ”€â”€ Output: [16, N, 256, 128, 240]
â””â”€â”€ Parameters: ~200,000 per neck
```

#### FPN Neck - Group2 & Group4
```
stage1_neck1 & stage1_neck3 (FpnNeck):
â”œâ”€â”€ Input Channels: [256, 512]
â”œâ”€â”€ Output Channels: 256
â”œâ”€â”€ Input Features:
â”‚   â”œâ”€â”€ P3: [16, N, 256, 64, 120]
â”‚   â””â”€â”€ P4: [16, N, 512, 32, 60]
â”œâ”€â”€ Output: [16, N, 256, 64, 120]
â””â”€â”€ Parameters: ~150,000 per neck
```

### ğŸš€ LiDARå¤„ç†åˆ†æ”¯

#### VFE (Voxel Feature Encoder)
```
lidar_vfe (PillarVFE_TA_va):
â”œâ”€â”€ Input: [16, 20000, 48, 4]  # è¿™é‡Œçš„16ä»£è¡¨çš„å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿç‚¹äº‘æä¾›äº†å“ªäº›ç‰¹å¾ï¼Ÿï¼Ÿ
â”œâ”€â”€ Voxel Size: [0.2, 0.2, 8.0]
â”œâ”€â”€ Point Cloud Range: [0, -44.8, -3.0, 112, 44.8, 5.0]
â”œâ”€â”€ Max Points per Voxel: 48
â”œâ”€â”€ Max Voxels: 20,000
â”œâ”€â”€ Feature Extraction:
â”‚   â”œâ”€â”€ Absolute XYZ: [x, y, z]
â”‚   â”œâ”€â”€ Distance: sqrt(xÂ² + yÂ² + zÂ²)
â”‚   â”œâ”€â”€ Point Count: actual points in voxel
â”‚   â””â”€â”€ TA (Temporal Attention) Features: 64-dim
â”œâ”€â”€ Output: [16, 20000, 64]
â””â”€â”€ Parameters: ~100,000
```

#### PointPillar Scatter
```
lidar_middle_encoder (PointPillarScatter):
â”œâ”€â”€ Input: [16, 20000, 64]
â”œâ”€â”€ Voxel Coords: [16, 20000, 4] [batch_idx, z, y, x]
â”œâ”€â”€ Grid Size: [896, 448, 1] (WÃ—HÃ—1)
â”œâ”€â”€ Output: [16, 64, 448, 896]
â”œâ”€â”€ Description: Scatter voxel features to BEV grid
â””â”€â”€ Parameters: Minimal (just indexing)
```

#### LiDAR Backbone
```
lidar_bev_backbone (ATBackbone):
â”œâ”€â”€ Input: [16, 64, 448, 896]
â”œâ”€â”€ Layer Configuration:
â”‚   â”œâ”€â”€ Layer1: [64â†’64], kernel=3, stride=1, layers=3
â”‚   â”‚   â”œâ”€â”€ Input: [16, 64, 448, 896]
â”‚   â”‚   â””â”€â”€ Output: [16, 64, 448, 896]
â”‚   â”œâ”€â”€ Layer2: [64â†’64], kernel=3, stride=2, layers=6
â”‚   â”‚   â”œâ”€â”€ Input: [16, 64, 448, 896]
â”‚   â”‚   â””â”€â”€ Output: [16, 64, 224, 448]
â”‚   â”œâ”€â”€ Layer3: [64â†’128], kernel=3, stride=2, layers=10
â”‚   â”‚   â”œâ”€â”€ Input: [16, 64, 224, 448]
â”‚   â”‚   â””â”€â”€ Output: [16, 128, 112, 224]
â”‚   â””â”€â”€ Layer4: [128â†’256], kernel=3, stride=2, layers=10
â”‚       â”œâ”€â”€ Input: [16, 128, 112, 224]
â”‚       â””â”€â”€ Output: [16, 256, 56, 112]
â”œâ”€â”€ Upsample Layers:
â”‚   â”œâ”€â”€ Upsample1: [256â†’128], stride=2
â”‚   â”œâ”€â”€ Upsample2: [128â†’128], stride=2
â”‚   â”œâ”€â”€ Upsample3: [128â†’128], stride=2
â”‚   â””â”€â”€ Upsample4: [128â†’128], stride=0.25
â”œâ”€â”€ Output Features: Multi-scale [64, 64, 128, 256, 128, 128, 128, 128]
â””â”€â”€ Parameters: ~2.0M
```

### ğŸ”„ è§†å›¾å˜æ¢æ¨¡å— (View Transfer)

#### BEV IPM Transfer
```
view_transfer (BevIpmTransfer):
â”œâ”€â”€ Input Features:
â”‚   â”œâ”€â”€ Group1: [16, 1, 256, 128, 240]
â”‚   â”œâ”€â”€ Group2: [16, 1, 256, 64, 120]
â”‚   â”œâ”€â”€ Group3: [16, 1, 256, 128, 240]
â”‚   â””â”€â”€ Group4: [16, 4, 256, 64, 120]
â”œâ”€â”€ Camera Parameters:
â”‚   â”œâ”€â”€ Intrinsics (K): [16, 7, 3, 3]
â”‚   â”œâ”€â”€ Distortion: [16, 7, 5]
â”‚   â”œâ”€â”€ BEV2Cam: [16, 7, 4, 4]
â”‚   â””â”€â”€ BEV2Img: [16, 7, 3, 3]
â”œâ”€â”€ BEV Configuration:
â”‚   â”œâ”€â”€ Dynamic Range: [-40, -44.8, -3.0, 62.4, 44.8, 5.0]
â”‚   â”œâ”€â”€ Static Range: [-20.8, -22.4, -3.0, 62.4, 22.4, 5.0]
â”‚   â”œâ”€â”€ Dynamic BEV: [112, 208] (0.4m resolution)
â”‚   â””â”€â”€ Static BEV: [56, 104] (0.8m resolution)
â”œâ”€â”€ Reference Points Generation:
â”‚   â”œâ”€â”€ Dynamic: [16, 112, 208, 2]
â”‚   â”œâ”€â”€ Static: [16, 56, 104, 2]
â”‚   â””â”€â”€ Points per Pillar: 4
â”œâ”€â”€ Deformable Attention:
â”‚   â”œâ”€â”€ Num Levels: 1
â”‚   â”œâ”€â”€ Num Points: 8
â”‚   â”œâ”€â”€ Embed Dim: 256
â”‚   â””â”€â”€ Num Heads: 8
â”œâ”€â”€ Output:
â”‚   â”œâ”€â”€ Dynamic BEV: [16, 256, 112, 208]
â”‚   â””â”€â”€ Static BEV: [16, 256, 56, 104]
â””â”€â”€ Parameters: ~500,000
```

### ğŸ”€ å¤šæ¨¡æ€èåˆæ¨¡å— (Fusion Module)

#### BEV Fuser
```
fuser (BevFuser):
â”œâ”€â”€ Input:
â”‚   â”œâ”€â”€ Camera BEV: [16, 256, 112, 208]
â”‚   â””â”€â”€ LiDAR BEV: [16, 512, 56, 112]
â”œâ”€â”€ LiDAR Upsampling:
â”‚   â”œâ”€â”€ ConvTranspose2d: [512â†’256], kernel=2, stride=2
â”‚   â”œâ”€â”€ Input: [16, 512, 56, 112]
â”‚   â””â”€â”€ Output: [16, 256, 112, 224]
â”œâ”€â”€ LiDAR Cropping:
â”‚   â”œâ”€â”€ Crop to: [16, 256, 112, 208]
â”‚   â””â”€â”€ Align with Camera BEV
â”œâ”€â”€ Fusion Operation:
â”‚   â”œâ”€â”€ Concatenation: [16, 512, 112, 208]
â”‚   â”œâ”€â”€ Conv2d: [512â†’256], kernel=3, padding=1
â”‚   â”œâ”€â”€ BatchNorm2d: 256 channels
â”‚   â””â”€â”€ ReLU Activation
â”œâ”€â”€ Output: [16, 256, 112, 208]
â””â”€â”€ Parameters: ~400,000
```

### ğŸ¯ ä»»åŠ¡ä¸“ç”¨å¤„ç†å¤´

#### åŠ¨æ€æ£€æµ‹å¤´ (Dynamic Head)
```
bev_dynamic_head (DynamicHead_Bin):
â”œâ”€â”€ Input: [16, 256, 112, 208]
â”œâ”€â”€ Neck Processing:
â”‚   â”œâ”€â”€ bev_dynamic_neck (ConvResBlockNeck):
â”‚   â”‚   â”œâ”€â”€ Input: [16, 256, 112, 208]
â”‚   â”‚   â”œâ”€â”€ Conv Blocks: 3Ã—[Conv3x3+BN+ReLU]
â”‚   â”‚   â”œâ”€â”€ Residual Connections: Yes
â”‚   â”‚   â”œâ”€â”€ Output: [16, 256, 112, 208]
â”‚   â”‚   â””â”€â”€ Parameters: ~200,000
â”‚   â”œâ”€â”€ Flatten: [16, 256, 112, 208] â†’ [16, 23328, 256]
â”‚   â””â”€â”€ Permute: [16, 23328, 256] â†’ [16, 23328, 256]
â”œâ”€â”€ Query Embedding:
â”‚   â”œâ”€â”€ Num Queries: 384
â”‚   â”œâ”€â”€ Embed Dim: 256
â”‚   â”œâ”€â”€ Learnable Parameters: [384, 256]
â”‚   â””â”€â”€ Parameters: ~98,000
â”œâ”€â”€ Transformer Decoder (StreamDetrDecoder):
â”‚   â”œâ”€â”€ Num Layers: 3
â”‚   â”œâ”€â”€ Each Layer (StreamTransformerLayer):
â”‚   â”‚   â”œâ”€â”€ Self-Attention:
â”‚   â”‚   â”‚   â”œâ”€â”€ MultiheadAttention: embed_dims=256, num_heads=8
â”‚   â”‚   â”‚   â”œâ”€â”€ Dropout: 0.1
â”‚   â”‚   â”‚   â””â”€â”€ Parameters: ~260,000
â”‚   â”‚   â”œâ”€â”€ Cross-Attention:
â”‚   â”‚   â”‚   â”œâ”€â”€ StreamDetrDeformableAttention
â”‚   â”‚   â”‚   â”œâ”€â”€ Num Levels: 1
â”‚   â”‚   â”‚   â”œâ”€â”€ Num Points: 20
â”‚   â”‚   â”‚   â”œâ”€â”€ WL Size: 20
â”‚   â”‚   â”‚   â””â”€â”€ Parameters: ~200,000
â”‚   â”‚   â”œâ”€â”€ FFN:
â”‚   â”‚   â”‚   â”œâ”€â”€ Linear: 256â†’512
â”‚   â”‚   â”‚   â”œâ”€â”€ ReLU
â”‚   â”‚   â”‚   â”œâ”€â”€ Linear: 512â†’256
â”‚   â”‚   â”‚   â”œâ”€â”€ Dropout: 0.1
â”‚   â”‚   â”‚   â””â”€â”€ Parameters: ~400,000
â”‚   â”‚   â””â”€â”€ LayerNorm: 256
â”‚   â””â”€â”€ Total Decoder Parameters: ~2.5M
â”œâ”€â”€ Prediction Heads:
â”‚   â”œâ”€â”€ Classification Head:
â”‚   â”‚   â”œâ”€â”€ Linear: 256â†’8
â”‚   â”‚   â”œâ”€â”€ Output: [16, 384, 8]
â”‚   â”‚   â””â”€â”€ Classes: car, truck, bus, person, non_motor, riderless_non_motor, barrier, pillar
â”‚   â”œâ”€â”€ Bbox Head:
â”‚   â”‚   â”œâ”€â”€ Linear: 256â†’6
â”‚   â”‚   â”œâ”€â”€ Output: [16, 384, 6]
â”‚   â”‚   â””â”€â”€ Format: [x, y, z, w, l, h]
â”‚   â”œâ”€â”€ Velocity Head:
â”‚   â”‚   â”œâ”€â”€ Linear: 256â†’3
â”‚   â”‚   â”œâ”€â”€ Output: [16, 384, 3]
â”‚   â”‚   â””â”€â”€ Format: [vx, vy, vz]
â”‚   â”œâ”€â”€ Bin Classification Head:
â”‚   â”‚   â”œâ”€â”€ Linear: 256â†’8
â”‚   â”‚   â”œâ”€â”€ Output: [16, 384, 8]
â”‚   â”‚   â””â”€â”€ Bins: 8 directional bins
â”‚   â””â”€â”€ Occlusion Head:
â”‚       â”œâ”€â”€ Linear: 256â†’1
â”‚       â”œâ”€â”€ Output: [16, 384, 1]
â”‚       â””â”€â”€ Levels: 4 occlusion levels
â”œâ”€â”€ Hungarian Assigner:
â”‚   â”œâ”€â”€ Classification Cost: FocalLossCost
â”‚   â”œâ”€â”€ Bbox Cost: L1Loss
â”‚   â””â”€â”€ Parameters: Minimal
â””â”€â”€ Total Dynamic Head Parameters: ~3.5M
```

#### é™æ€åˆ†å‰²å¤´ (Static Head)
```
bev_static_head (StaticSegHead):
â”œâ”€â”€ Input: [16, 256, 56, 104]
â”œâ”€â”€ Neck Processing:
â”‚   â”œâ”€â”€ bev_static_neck (Conv3x3Neck):
â”‚   â”‚   â”œâ”€â”€ Input: [16, 256, 56, 104]
â”‚   â”‚   â”œâ”€â”€ Conv3x3: 256â†’128
â”‚   â”‚   â”œâ”€â”€ BatchNorm + ReLU
â”‚   â”‚   â”œâ”€â”€ Output: [16, 128, 56, 104]
â”‚   â”‚   â””â”€â”€ Parameters: ~300,000
â”‚   â””â”€â”€ Upsample: [16, 128, 56, 104] â†’ [16, 128, 224, 416]
â”œâ”€â”€ Lane Marking Head:
â”‚   â”œâ”€â”€ Input: [16, 128, 224, 416]
â”‚   â”œâ”€â”€ Conv Blocks: 5Ã—[Conv3x3+BN+ReLU]
â”‚   â”œâ”€â”€ Output Channels: 9
â”‚   â”œâ”€â”€ Output: [16, 9, 224, 416]
â”‚   â”œâ”€â”€ Classes: SolidLine, DoubleSolidLine, DashedLine, DoubleDashedLine, 
â”‚   â”‚             RightSolidLeftDashed, LeftSolidRightDashed, 
â”‚   â”‚             WideSolidLine, WideDashedLine, ShortDashedLine
â”‚   â””â”€â”€ Parameters: ~500,000
â”œâ”€â”€ Road Element Head:
â”‚   â”œâ”€â”€ Input: [16, 128, 224, 416]
â”‚   â”œâ”€â”€ Conv Blocks: 5Ã—[Conv3x3+BN+ReLU]
â”‚   â”œâ”€â”€ Output Channels: 4
â”‚   â”œâ”€â”€ Output: [16, 4, 224, 416]
â”‚   â”œâ”€â”€ Classes: Wall, Curb, Lane, SpeedBump, GroundSigns
â”‚   â””â”€â”€ Parameters: ~200,000
â”œâ”€â”€ Instance Detection Head:
â”‚   â”œâ”€â”€ Input: [16, 128, 224, 416]
â”‚   â”œâ”€â”€ Heatmap Head:
â”‚   â”‚   â”œâ”€â”€ Conv Blocks: 3Ã—[Conv3x3+BN+ReLU]
â”‚   â”‚   â”œâ”€â”€ Output: [16, 1, 224, 416]
â”‚   â”‚   â””â”€â”€ Max Instances: 100
â”‚   â”œâ”€â”€ Regression Head:
â”‚   â”‚   â”œâ”€â”€ Conv Blocks: 3Ã—[Conv3x3+BN+ReLU]
â”‚   â”‚   â”œâ”€â”€ Output: [16, 6, 224, 416]
â”‚   â”‚   â””â”€â”€ Format: [offset_x, offset_y, w, h, angle, class]
â”‚   â””â”€â”€ Total Instance Parameters: ~400,000
â””â”€â”€ Total Static Head Parameters: ~1.4M
```

## ğŸ“Š å®Œæ•´å‚æ•°ç»Ÿè®¡

### æ€»å‚æ•°é‡åˆ†å¸ƒ
```
Backbone Networks:
â”œâ”€â”€ stage1_backbone0: ~21.3M
â”œâ”€â”€ stage1_backbone1: ~21.3M
â”œâ”€â”€ stage1_backbone2: ~21.3M
â”œâ”€â”€ stage1_backbone3: ~21.3M
â””â”€â”€ Backbone Total: ~85.2M

Neck Networks:
â”œâ”€â”€ stage1_neck0: ~200,000
â”œâ”€â”€ stage1_neck1: ~150,000
â”œâ”€â”€ stage1_neck2: ~200,000
â”œâ”€â”€ stage1_neck3: ~150,000
â””â”€â”€ Neck Total: ~700,000

LiDAR Networks:
â”œâ”€â”€ lidar_vfe: ~100,000
â”œâ”€â”€ lidar_middle_encoder: ~1,000
â”œâ”€â”€ lidar_bev_backbone: ~2.0M
â””â”€â”€ LiDAR Total: ~2.1M

Fusion & View Transfer:
â”œâ”€â”€ view_transfer: ~500,000
â”œâ”€â”€ fuser: ~400,000
â””â”€â”€ Fusion Total: ~900,000

Task Heads:
â”œâ”€â”€ bev_dynamic_head: ~3.5M
â”œâ”€â”€ bev_static_head: ~1.4M
â””â”€â”€ Heads Total: ~4.9M

Grand Total Parameters: ~93.8M
```

### å†…å­˜å ç”¨åˆ†æ (Batch=16)
```
Input Memory:
â”œâ”€â”€ Camera Images: ~150MB
â”œâ”€â”€ LiDAR Data: ~61MB
â””â”€â”€ Input Total: ~211MB

Feature Memory:
â”œâ”€â”€ Backbone Features: ~2.7GB
â”œâ”€â”€ Neck Features: ~800MB
â”œâ”€â”€ BEV Features: ~400MB
â””â”€â”€ Feature Total: ~3.9GB

Output Memory:
â”œâ”€â”€ Dynamic Predictions: ~50MB
â”œâ”€â”€ Static Predictions: ~100MB
â””â”€â”€ Output Total: ~150MB

Total Memory Usage: ~4.3GB per GPU
```

## ğŸ”„ å®Œæ•´å‰å‘ä¼ æ’­æµç¨‹

### Step 1: æ•°æ®é¢„å¤„ç†
```
Raw Input:
â”œâ”€â”€ Camera Images: [16, 7, 3, H, W]
â”œâ”€â”€ LiDAR Points: Variable length
â””â”€â”€ Camera/LiDAR Parameters

Preprocessing:
â”œâ”€â”€ Image Resize: HÃ—W â†’ 512Ã—960
â”œâ”€â”€ Normalization: (img - 128) / 1.0
â”œâ”€â”€ Data Augmentation: 2D/3D transforms
â””â”€â”€ Voxelization: Points â†’ Voxels
```

### Step 2: ç‰¹å¾æå–
```
Camera Branch:
â”œâ”€â”€ Group Processing: 4 parallel ResNet34
â”œâ”€â”€ Multi-scale Features: P2, P3, P4
â”œâ”€â”€ FPN Fusion: Top-down + Lateral
â””â”€â”€ Camera Features: [16, 7, 256, H/8, W/8]

LiDAR Branch:
â”œâ”€â”€ VFE: Point â†’ Voxel Features
â”œâ”€â”€ Scatter: Voxel â†’ BEV Grid
â”œâ”€â”€ Backbone: Multi-scale 2D CNN
â””â”€â”€ LiDAR Features: [16, 512, 56, 112]
```

### Step 3: BEVå˜æ¢
```
View Transfer:
â”œâ”€â”€ Reference Points: BEV grid sampling
â”œâ”€â”€ Deformable Attention: Feature sampling
â”œâ”€â”€ Multi-camera Fusion: 7â†’1
â””â”€â”€ BEV Features: Dynamic + Static

Fusion:
â”œâ”€â”€ LiDAR Upsampling: Match camera resolution
â”œâ”€â”€ Feature Concatenation: Camera + LiDAR
â”œâ”€â”€ Fusion Conv: 512â†’256
â””â”€â”€ Fused BEV: [16, 256, 112, 208]
```

### Step 4: ä»»åŠ¡å¤„ç†
```
Dynamic Task:
â”œâ”€â”€ Neck: ConvResBlock processing
â”œâ”€â”€ Transformer: Query-based decoding
â”œâ”€â”€ Multi-head Prediction: 384 queries
â””â”€â”€ Outputs: cls, bbox, velo, bin, occlude

Static Task:
â”œâ”€â”€ Neck: Conv3x3 processing
â”œâ”€â”€ Multi-task Heads: Lane + Road + Instance
â”œâ”€â”€ Upsampling: 2Ã— resolution
â””â”€â”€ Outputs: seg_maps, instance_dets
```

### Step 5: æŸå¤±è®¡ç®—
```
Dynamic Losses:
â”œâ”€â”€ Classification Loss: FocalLoss (Î³=2.0, Î±=0.25)
â”œâ”€â”€ Bbox Loss: L1Loss
â”œâ”€â”€ Bin Loss: CrossEntropyLoss
â”œâ”€â”€ Occlusion Loss: FocalLoss
â”œâ”€â”€ Velocity Loss: L1Loss
â””â”€â”€ Weighted Sum: total_loss = Î£(wi Ã— lossi)

Static Losses:
â”œâ”€â”€ Lane Segmentation Loss: FocalLoss + DiceLoss
â”œâ”€â”€ Road Segmentation Loss: FocalLoss + DiceLoss
â”œâ”€â”€ Instance Detection Loss: CenterNetLoss
â””â”€â”€ Weighted Sum: total_loss = Î£(wi Ã— lossi)
```

## ğŸ¯ ç½‘ç»œè¾“å‡ºè¯¦ç»†è¯´æ˜

### åŠ¨æ€ä»»åŠ¡è¾“å‡º
```
Detection Results (per query):
â”œâ”€â”€ Classification: [16, 384, 8]
â”‚   â”œâ”€â”€ Format: [car, truck, bus, person, non_motor, riderless_non_motor, barrier, pillar]
â”‚   â”œâ”€â”€ Activation: Sigmoid
â”‚   â””â”€â”€ Threshold: 0.3 (default)
â”œâ”€â”€ Bounding Box: [16, 384, 6]
â”‚   â”œâ”€â”€ Format: [x, y, z, w, l, h]
â”‚   â”œâ”€â”€ Coordinate: BEV (ego vehicle)
â”‚   â””â”€â”€ Units: meters
â”œâ”€â”€ Velocity: [16, 384, 3]
â”‚   â”œâ”€â”€ Format: [vx, vy, vz]
â”‚   â”œâ”€â”€ Coordinate: BEV
â”‚   â””â”€â”€ Units: m/s
â”œâ”€â”€ Bin Classification: [16, 384, 8]
â”‚   â”œâ”€â”€ Format: 8 directional bins
â”‚   â”œâ”€â”€ Angle Range: [-Ï€, Ï€]
â”‚   â””â”€â”€ Bin Width: Ï€/4
â””â”€â”€ Occlusion: [16, 384, 1]
    â”œâ”€â”€ Format: [NoOccluded, SlightlyOccluded, PartlyOccluded, HeavilyOccluded]
    â””â”€â”€ Activation: Sigmoid
```

### é™æ€ä»»åŠ¡è¾“å‡º
```
Segmentation Results:
â”œâ”€â”€ Lane Marking: [16, 9, 224, 416]
â”‚   â”œâ”€â”€ Resolution: 0.2m Ã— 0.2m
â”‚   â”œâ”€â”€ Classes: 9 lane marking types
â”‚   â””â”€â”€ Activation: Sigmoid
â”œâ”€â”€ Road Elements: [16, 4, 224, 416]
â”‚   â”œâ”€â”€ Resolution: 0.2m Ã— 0.2m
â”‚   â”œâ”€â”€ Classes: [Wall, Curb, Lane, SpeedBump, GroundSigns]
â”‚   â””â”€â”€ Activation: Sigmoid
â””â”€â”€ Instance Detection: [16, 100, 6]
    â”œâ”€â”€ Format: [offset_x, offset_y, w, h, angle, class]
    â”œâ”€â”€ Max Instances: 100
    â””â”€â”€ Coordinate: BEV
```

è¿™ä¸ªè¯¦ç»†çš„ç½‘ç»œæ¶æ„è¾“å‡ºå±•ç¤ºäº†LeapAIæ¡†æ¶åœ¨æ‚¨é…ç½®ä¸‹çš„å®Œæ•´ç»“æ„ï¼ŒåŒ…æ‹¬æ¯ä¸€å±‚çš„å‚æ•°é‡ã€æ•°æ®æµåŠ¨å’Œå†…å­˜å ç”¨ã€‚
