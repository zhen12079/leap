#!/usr/bin/env python3
"""
LeapAIæ¡†æ¶å­¦ä¹  - é˜¶æ®µ3ï¼šæ•°æ®æ¨¡å—å’Œæ•°æ®å¤„ç†æµç¨‹å®è·µ

æœ¬é˜¶æ®µå­¦ä¹ ç›®æ ‡ï¼š
1. ç†è§£å¤šä»»åŠ¡æ•°æ®åŠ è½½æœºåˆ¶
2. å­¦ä¹ æ•°æ®é¢„å¤„ç†å’Œå¢å¼ºæµç¨‹
3. æŒæ¡ç›®æ ‡ç”Ÿæˆå’Œæ ‡ç­¾å¤„ç†
4. å®è·µæ•°æ®æ¨¡å—çš„é…ç½®å’Œä½¿ç”¨
"""

import os
import sys
import torch
import numpy as np
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

def explore_data_module_architecture():
    """æ¢ç´¢æ•°æ®æ¨¡å—æ¶æ„"""
    
    print("=" * 60)
    print("ğŸ“Š æ•°æ®æ¨¡å—æ¶æ„æ¢ç´¢")
    print("=" * 60)
    
    try:
        # è¯»å–æ•°æ®æ¨¡å—æ ¸å¿ƒæ–‡ä»¶
        data_module_path = "leapai/data/data_module.py"
        
        if not os.path.exists(data_module_path):
            print(f"âŒ æ•°æ®æ¨¡å—æ–‡ä»¶ä¸å­˜åœ¨: {data_module_path}")
            return False
        
        with open(data_module_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print("âœ… æ•°æ®æ¨¡å—æ ¸å¿ƒæ–‡ä»¶è¯»å–æˆåŠŸ")
        
        # åˆ†æå…³é”®ç±»å’Œæ–¹æ³•
        key_classes = []
        key_methods = []
        
        lines = content.split('\n')
        for line in lines:
            line = line.strip()
            if line.startswith('class '):
                class_name = line.split('(')[0].replace('class ', '').strip(':')
                key_classes.append(class_name)
            elif 'def ' in line and not line.startswith('#'):
                method_name = line.split('(')[0].strip().replace('def ', '')
                if not method_name.startswith('_'):
                    key_methods.append(method_name)
        
        print(f"\nğŸ“‹ å‘ç°çš„å…³é”®ç±»:")
        for i, cls in enumerate(key_classes, 1):
            print(f"  {i}. {cls}")
        
        print(f"\nğŸ“‹ å‘ç°çš„å…¬å…±æ–¹æ³•:")
        for i, method in enumerate(key_methods[:10], 1):  # åªæ˜¾ç¤ºå‰10ä¸ª
            print(f"  {i}. {method}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æ¨¡å—æ¶æ„æ¢ç´¢å¤±è´¥: {e}")
        return False

def analyze_dataloader_components():
    """åˆ†ææ•°æ®åŠ è½½å™¨ç»„ä»¶"""
    
    print("\n" + "=" * 60)
    print("ğŸ”„ æ•°æ®åŠ è½½å™¨ç»„ä»¶åˆ†æ")
    print("=" * 60)
    
    try:
        # æŸ¥çœ‹æ•°æ®åŠ è½½å™¨ç›®å½•
        dataloader_dir = "leapai/data/dataloader"
        
        if os.path.exists(dataloader_dir):
            dataloader_files = [f for f in os.listdir(dataloader_dir) if f.endswith('.py')]
            print(f"ğŸ“ æ•°æ®åŠ è½½å™¨æ–‡ä»¶ ({len(dataloader_files)}ä¸ª):")
            for i, file in enumerate(dataloader_files, 1):
                print(f"  {i}. {file}")
        
        # åˆ†æå…·ä½“çš„æ•°æ®åŠ è½½å™¨å®ç°
        key_loaders = [
            "combined_dataloader.py",
            "cycle_iterator.py"
        ]
        
        print(f"\nğŸ” å…³é”®æ•°æ®åŠ è½½å™¨åˆ†æ:")
        for loader in key_loaders:
            loader_path = f"leapai/data/dataloader/{loader}"
            if os.path.exists(loader_path):
                with open(loader_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æå–ç±»ä¿¡æ¯
                lines = content.split('\n')
                classes = []
                for line in lines:
                    if line.strip().startswith('class '):
                        class_name = line.split('(')[0].replace('class ', '').strip(':')
                        classes.append(class_name)
                
                print(f"  â€¢ {loader}: {classes}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å™¨åˆ†æå¤±è´¥: {e}")
        return False

def explore_dataset_implementations():
    """æ¢ç´¢æ•°æ®é›†å®ç°"""
    
    print("\n" + "=" * 60)
    print("ğŸ“‚ æ•°æ®é›†å®ç°æ¢ç´¢")
    print("=" * 60)
    
    try:
        # æŸ¥çœ‹æ•°æ®é›†ç›®å½•
        dataset_dir = "leapai/data/dataset"
        
        if os.path.exists(dataset_dir):
            dataset_files = [f for f in os.listdir(dataset_dir) if f.endswith('.py')]
            print(f"ğŸ“ æ•°æ®é›†æ–‡ä»¶ ({len(dataset_files)}ä¸ª):")
            for i, file in enumerate(dataset_files, 1):
                print(f"  {i}. {file}")
        
        # åˆ†æå…³é”®æ•°æ®é›†ç±»å‹
        dataset_types = {
            "bev_dataset.py": "BEVæ•°æ®é›†",
            "fusion_dataset.py": "èåˆæ•°æ®é›†", 
            "lidar_dataset.py": "LiDARæ•°æ®é›†",
            "video_iterable_dataset.py": "è§†é¢‘æ•°æ®é›†"
        }
        
        print(f"\nğŸ¯ å…³é”®æ•°æ®é›†ç±»å‹:")
        for filename, description in dataset_types.items():
            filepath = f"leapai/data/dataset/{filename}"
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æå–ç±»ä¿¡æ¯
                lines = content.split('\n')
                classes = []
                for line in lines:
                    if line.strip().startswith('class '):
                        class_name = line.split('(')[0].replace('class ', '').strip(':')
                        classes.append(class_name)
                
                print(f"  â€¢ {description} ({filename}): {classes}")
        
        # æŸ¥çœ‹perceptioné¡¹ç›®çš„æ•°æ®é›†å®ç°
        perception_dataset_dir = "projects/perception/dataset"
        if os.path.exists(perception_dataset_dir):
            perception_files = [f for f in os.listdir(perception_dataset_dir) if f.endswith('.py')]
            print(f"\nğŸš— Perceptioné¡¹ç›®æ•°æ®é›† ({len(perception_files)}ä¸ª):")
            for i, file in enumerate(perception_files, 1):
                print(f"  {i}. {file}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®é›†æ¢ç´¢å¤±è´¥: {e}")
        return False

def analyze_data_transforms():
    """åˆ†ææ•°æ®å˜æ¢"""
    
    print("\n" + "=" * 60)
    print("ğŸ”„ æ•°æ®å˜æ¢åˆ†æ")
    print("=" * 60)
    
    try:
        # æŸ¥çœ‹æ•°æ®å˜æ¢ç›®å½•
        transform_dir = "leapai/data/transform"
        
        if os.path.exists(transform_dir):
            transform_files = [f for f in os.listdir(transform_dir) if f.endswith('.py')]
            print(f"ğŸ“ æ•°æ®å˜æ¢æ–‡ä»¶ ({len(transform_files)}ä¸ª):")
            for i, file in enumerate(transform_files, 1):
                print(f"  {i}. {file}")
        
        # åˆ†æå…³é”®å˜æ¢ç±»å‹
        transform_types = {
            "augment.py": "æ•°æ®å¢å¼º",
            "lidar_augment.py": "LiDARå¢å¼º",
            "image_tensor_transfer.py": "å›¾åƒå˜æ¢",
            "lidar_processor.py": "LiDARå¤„ç†",
            "point2voxel.py": "ç‚¹äº‘ä½“ç´ åŒ–"
        }
        
        print(f"\nğŸ”§ å…³é”®å˜æ¢ç±»å‹:")
        for filename, description in transform_types.items():
            filepath = f"leapai/data/transform/{filename}"
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # ç»Ÿè®¡å‡½æ•°æ•°é‡
                lines = content.split('\n')
                functions = []
                for line in lines:
                    if line.strip().startswith('def '):
                        func_name = line.split('(')[0].strip().replace('def ', '')
                        if not func_name.startswith('_'):
                            functions.append(func_name)
                
                print(f"  â€¢ {description} ({filename}): {len(functions)} ä¸ªå‡½æ•°")
                for func in functions[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"    - {func}")
                if len(functions) > 3:
                    print(f"    - ... è¿˜æœ‰ {len(functions) - 3} ä¸ªå‡½æ•°")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®å˜æ¢åˆ†æå¤±è´¥: {e}")
        return False

def explore_target_generation():
    """æ¢ç´¢ç›®æ ‡ç”Ÿæˆ"""
    
    print("\n" + "=" * 60)
    print("ğŸ¯ ç›®æ ‡ç”Ÿæˆæ¢ç´¢")
    print("=" * 60)
    
    try:
        # æŸ¥çœ‹ç›®æ ‡ç”Ÿæˆç›®å½•
        target_dir = "leapai/data/target"
        
        if os.path.exists(target_dir):
            target_files = [f for f in os.listdir(target_dir) if f.endswith('.py')]
            print(f"ğŸ“ ç›®æ ‡ç”Ÿæˆæ–‡ä»¶ ({len(target_files)}ä¸ª):")
            for i, file in enumerate(target_files, 1):
                print(f"  {i}. {file}")
        
        # åˆ†æç›®æ ‡ç±»å‹
        target_types = {
            "bev_dynamic_target.py": "BEVåŠ¨æ€ç›®æ ‡",
            "bev_static_target.py": "BEVé™æ€ç›®æ ‡",
            "lidar_det_target.py": "LiDARæ£€æµ‹ç›®æ ‡",
            "lidar_lane_target.py": "LiDARè½¦é“çº¿ç›®æ ‡",
            "lidar_seg_target.py": "LiDARåˆ†å‰²ç›®æ ‡"
        }
        
        print(f"\nğŸ¯ ç›®æ ‡ç±»å‹åˆ†æ:")
        for filename, description in target_types.items():
            filepath = f"leapai/data/target/{filename}"
            if os.path.exists(filepath):
                with open(filepath, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æå–ç±»ä¿¡æ¯
                lines = content.split('\n')
                classes = []
                for line in lines:
                    if line.strip().startswith('class '):
                        class_name = line.split('(')[0].replace('class ', '').strip(':')
                        classes.append(class_name)
                
                print(f"  â€¢ {description} ({filename}): {classes}")
        
        return True
        
    except Exception as e:
        print(f"âŒ ç›®æ ‡ç”Ÿæˆæ¢ç´¢å¤±è´¥: {e}")
        return False

def practice_data_configuration():
    """å®è·µæ•°æ®é…ç½®"""
    
    print("\n" + "=" * 60)
    print("âš™ï¸ æ•°æ®é…ç½®å®è·µ")
    print("=" * 60)
    
    try:
        # åˆ†æé…ç½®æ–‡ä»¶ä¸­çš„æ•°æ®é…ç½®
        config_path = "projects/perception/configs/lpperception_current_hpa_step1.py"
        
        if not os.path.exists(config_path):
            print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
            return False
        
        # è¯»å–é…ç½®æ–‡ä»¶
        with open(config_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print("âœ… é…ç½®æ–‡ä»¶è¯»å–æˆåŠŸ")
        
        # æŸ¥æ‰¾æ•°æ®ç›¸å…³é…ç½®
        data_sections = []
        lines = content.split('\n')
        current_section = []
        in_data_section = False
        
        for line in lines:
            if any(keyword in line for keyword in ['data_module', 'dataset', 'dataloader', 'transform']):
                in_data_section = True
                current_section = [line]
            elif in_data_section:
                if line.strip() == '' or (line.startswith(' ') == False and not line.startswith('\t')):
                    if current_section:
                        data_sections.append('\n'.join(current_section))
                        current_section = []
                    in_data_section = False
                else:
                    current_section.append(line)
        
        if current_section:
            data_sections.append('\n'.join(current_section))
        
        print(f"\nğŸ“‹ å‘ç°çš„æ•°æ®é…ç½®æ®µ ({len(data_sections)}ä¸ª):")
        for i, section in enumerate(data_sections, 1):
            lines = section.split('\n')
            title = lines[0].strip() if lines else "Unknown"
            print(f"  {i}. {title}")
            # æ˜¾ç¤ºå‰å‡ è¡Œå†…å®¹
            for line in lines[1:4]:
                if line.strip():
                    print(f"     {line.strip()}")
            print()
        
        # åˆ›å»ºç¤ºä¾‹æ•°æ®é…ç½®
        example_config = {
            "data_module": {
                "type": "MultiTaskDataModule",
                "dataset_cfg": {
                    "type": "FusionDataset",
                    "data_root": "/path/to/data",
                    "train_split": "train.txt",
                    "val_split": "val.txt"
                },
                "dataloader_cfg": {
                    "batch_size": 8,
                    "num_workers": 4,
                    "pin_memory": True
                },
                "transform_cfg": {
                    "train": ["RandomFlip", "RandomScale", "Normalize"],
                    "val": ["Normalize"]
                }
            }
        }
        
        print("âœ… ç¤ºä¾‹æ•°æ®é…ç½®:")
        def print_config(config, indent=0):
            for key, value in config.items():
                if isinstance(value, dict):
                    print("  " * indent + f"â€¢ {key}:")
                    print_config(value, indent + 1)
                else:
                    print("  " * indent + f"  {key}: {value}")
        
        print_config(example_config)
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®é…ç½®å®è·µå¤±è´¥: {e}")
        return False

def simulate_data_pipeline():
    """æ¨¡æ‹Ÿæ•°æ®æµæ°´çº¿"""
    
    print("\n" + "=" * 60)
    print("ğŸ”„ æ•°æ®æµæ°´çº¿æ¨¡æ‹Ÿ")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ¨¡æ‹Ÿçš„æ•°æ®å¤„ç†æµç¨‹
        class MockDataPipeline:
            """æ¨¡æ‹Ÿçš„æ•°æ®å¤„ç†æµæ°´çº¿"""
            
            def __init__(self):
                self.steps = [
                    "æ•°æ®åŠ è½½",
                    "æ•°æ®é¢„å¤„ç†", 
                    "æ•°æ®å¢å¼º",
                    "ç›®æ ‡ç”Ÿæˆ",
                    "æ‰¹æ¬¡ç»„ç»‡"
                ]
            
            def process_batch(self, batch_size=4):
                """æ¨¡æ‹Ÿå¤„ç†ä¸€ä¸ªæ‰¹æ¬¡çš„æ•°æ®"""
                print(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡å¤§å°: {batch_size}")
                
                for i, step in enumerate(self.steps, 1):
                    print(f"  {i}. {step}...")
                    # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
                    import time
                    time.sleep(0.1)
                    print(f"     âœ… {step}å®Œæˆ")
                
                # æ¨¡æ‹Ÿè¾“å‡ºæ•°æ®
                mock_output = {
                    "images": torch.randn(batch_size, 3, 224, 224),
                    "lidar": torch.randn(batch_size, 1000, 4),
                    "targets": {
                        "dynamic": torch.randint(0, 10, (batch_size, 50)),
                        "static": torch.randint(0, 5, (batch_size, 100))
                    },
                    "metadata": ["frame_001", "frame_002", "frame_003", "frame_004"]
                }
                
                return mock_output
        
        # åˆ›å»ºå¹¶è¿è¡Œæ•°æ®æµæ°´çº¿
        pipeline = MockDataPipeline()
        output = pipeline.process_batch()
        
        print(f"\nğŸ“Š è¾“å‡ºæ•°æ®ç»“æ„:")
        for key, value in output.items():
            if isinstance(value, torch.Tensor):
                print(f"  â€¢ {key}: {value.shape} {value.dtype}")
            elif isinstance(value, dict):
                print(f"  â€¢ {key}:")
                for sub_key, sub_value in value.items():
                    if isinstance(sub_value, torch.Tensor):
                        print(f"    - {sub_key}: {sub_value.shape} {sub_value.dtype}")
            else:
                print(f"  â€¢ {key}: {type(value).__name__} ({len(value)} é¡¹)")
        
        print("âœ… æ•°æ®æµæ°´çº¿æ¨¡æ‹Ÿå®Œæˆ")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ•°æ®æµæ°´çº¿æ¨¡æ‹Ÿå¤±è´¥: {e}")
        return False

def show_learning_summary():
    """æ˜¾ç¤ºå­¦ä¹ æ€»ç»“"""
    
    print("\n" + "=" * 60)
    print("ğŸ“š é˜¶æ®µ3å­¦ä¹ æ€»ç»“")
    print("=" * 60)
    
    summary_points = [
        "ğŸ“Š æ•°æ®æ¨¡å—æ¶æ„ï¼šç†è§£äº†å¤šä»»åŠ¡æ•°æ®åŠ è½½çš„è®¾è®¡ç†å¿µ",
        "ğŸ”„ æ•°æ®åŠ è½½å™¨ï¼šæŒæ¡äº†å¤šç§æ•°æ®åŠ è½½å™¨çš„å®ç°å’Œä½¿ç”¨",
        "ğŸ“‚ æ•°æ®é›†å®ç°ï¼šå­¦ä¹ äº†ä¸åŒç±»å‹æ•°æ®é›†çš„å¤„ç†æ–¹å¼",
        "ğŸ”„ æ•°æ®å˜æ¢ï¼šæŒæ¡äº†æ•°æ®é¢„å¤„ç†å’Œå¢å¼ºæŠ€æœ¯",
        "ğŸ¯ ç›®æ ‡ç”Ÿæˆï¼šç†è§£äº†æ ‡ç­¾ç”Ÿæˆå’Œç›®æ ‡å¤„ç†æœºåˆ¶",
        "âš™ï¸ é…ç½®å®è·µï¼šå®è·µäº†æ•°æ®æ¨¡å—çš„é…ç½®å’Œä½¿ç”¨"
    ]
    
    for point in summary_points:
        print(f"  {point}")
    
    print("\nğŸ¯ ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®:")
    next_steps = [
        "1. å­¦ä¹ æ¨¡å‹æ„å»ºå’ŒNodeGraphæœºåˆ¶",
        "2. ç†è§£å¤šä»»åŠ¡è®­ç»ƒå’Œæ‹“æ‰‘å®šä¹‰",
        "3. æŒæ¡æ„ŸçŸ¥ä»»åŠ¡çš„å…·ä½“å®ç°",
        "4. å­¦ä¹ åˆ†å¸ƒå¼è®­ç»ƒå’Œéƒ¨ç½²"
    ]
    
    for step in next_steps:
        print(f"  {step}")
    
    print("\nğŸ’¡ å…³é”®æ–‡ä»¶å›é¡¾:")
    key_files = [
        "â€¢ leapai/data/data_module.py - æ•°æ®æ¨¡å—æ ¸å¿ƒ",
        "â€¢ leapai/data/dataloader/ - æ•°æ®åŠ è½½å™¨å®ç°",
        "â€¢ leapai/data/dataset/ - æ•°æ®é›†å®ç°",
        "â€¢ leapai/data/transform/ - æ•°æ®å˜æ¢å®ç°",
        "â€¢ leapai/data/target/ - ç›®æ ‡ç”Ÿæˆå®ç°"
    ]
    
    for file in key_files:
        print(f"  {file}")

def main():
    """ä¸»å‡½æ•°"""
    
    print("ğŸ“ LeapAIæ¡†æ¶å­¦ä¹  - é˜¶æ®µ3ï¼šæ•°æ®æ¨¡å—å’Œæ•°æ®å¤„ç†æµç¨‹")
    print("æœ¬é˜¶æ®µå°†æ·±å…¥ç†è§£LeapAIçš„æ•°æ®å¤„ç†æœºåˆ¶")
    
    try:
        # æ‰§è¡Œå­¦ä¹ æ­¥éª¤
        steps = [
            ("æ¢ç´¢æ•°æ®æ¨¡å—æ¶æ„", explore_data_module_architecture),
            ("åˆ†ææ•°æ®åŠ è½½å™¨ç»„ä»¶", analyze_dataloader_components),
            ("æ¢ç´¢æ•°æ®é›†å®ç°", explore_dataset_implementations),
            ("åˆ†ææ•°æ®å˜æ¢", analyze_data_transforms),
            ("æ¢ç´¢ç›®æ ‡ç”Ÿæˆ", explore_target_generation),
            ("å®è·µæ•°æ®é…ç½®", practice_data_configuration),
            ("æ¨¡æ‹Ÿæ•°æ®æµæ°´çº¿", simulate_data_pipeline)
        ]
        
        completed_steps = 0
        for step_name, step_func in steps:
            print(f"\nğŸ”„ æ‰§è¡Œæ­¥éª¤: {step_name}")
            if step_func():
                completed_steps += 1
                print(f"âœ… {step_name} å®Œæˆ")
            else:
                print(f"âŒ {step_name} å¤±è´¥")
        
        # æ˜¾ç¤ºå­¦ä¹ æ€»ç»“
        show_learning_summary()
        
        print(f"\nğŸ‰ é˜¶æ®µ3å­¦ä¹ å®Œæˆï¼")
        print(f"å®Œæˆæ­¥éª¤: {completed_steps}/{len(steps)}")
        
        return completed_steps == len(steps)
        
    except Exception as e:
        print(f"âŒ å­¦ä¹ è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
